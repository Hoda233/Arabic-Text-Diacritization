{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V_glxv-sQ7S",
        "outputId": "81fc459a-9392-4696-cb31-974f3d144690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NCPGYLnfsfCt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import Sequence\n",
        "from keras.initializers import glorot_normal\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfXJ-UcztpaD",
        "outputId": "68a303ef-18ec-4779-b9ea-9e59d40230e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-277be1330e1a>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "GPU device name: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU available:\", tf.test.is_gpu_available())\n",
        "print(\"GPU device name:\", tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40x1iLqltvac",
        "outputId": "191939ee-74fd-42d1-8851-1f9c1049f548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data length: 50000\n",
            "Validation data length: 2500\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/My Drive/NLPdata/train.txt','r') as file:\n",
        "    train_data = file.readlines()\n",
        "\n",
        "val_data_raw = None\n",
        "with open('/content/drive/My Drive/NLPdata/val.txt','r') as file:\n",
        "    val_data = file.readlines()\n",
        "\n",
        "print('Training data length:', len(train_data))\n",
        "print('Validation data length:', len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-XwE5IuntvwK"
      },
      "outputs": [],
      "source": [
        "diacritics = ['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ْ', 'ّ', 'َّ', 'ًّ', 'ُّ', 'ٌّ', 'ِّ', 'ٍّ', '']\n",
        "\n",
        "arabic_chars = ['ئ', 'ط', 'ه', 'ك', 'ض', 'ج', 'ذ', 'ع', 'ب', 'ل', 'د', 'ت', 'ا', 'ث', 'أ', 'س', 'ق', 'م', 'ش', 'ز', 'غ', 'ى', 'إ', 'خ', 'ن', 'آ', 'ؤ', 'ي', 'ظ', 'ص', 'ح', 'ة', 'و', 'ر', 'ء', 'ف']\n",
        "arabic_chars_space = list(arabic_chars) + [' ']\n",
        "arabic_chars_valid = list(arabic_chars) + [' '] + diacritics\n",
        "\n",
        "# char_mapping = {' ': 0,\n",
        "#     'ا': 1, 'ب': 2, 'ت': 3, 'ث': 4, 'ج': 5, 'ح': 6, 'خ': 7, 'د': 8, 'ذ': 9, 'ر': 10, 'ز': 11, 'س': 12, 'ش': 13, 'ص': 14,\n",
        "#     'ض': 15, 'ط': 16, 'ظ': 17, 'ع': 18, 'غ': 19, 'ف': 20, 'ق': 21, 'ك': 22, 'ل': 23, 'م': 24, 'ن': 25, 'ه': 26, 'و': 27,\n",
        "#     'ى': 28, 'ي': 29,'ء': 30, 'آ': 31, 'أ': 32, 'ؤ': 33, 'إ': 34, 'ئ': 35,'ة': 36,\n",
        "#     '٠': 37, '١': 38, '٢': 39, '٣': 40, '٤': 41, '٥': 42, '٦': 43, '٧': 44, '٨': 45, '٩': 46,\n",
        "#     '0': 47, '1': 48, '2': 49, '3': 50, '4': 51, '5': 52, '6': 53,'7': 54, '8': 55, '9': 56,\n",
        "#     '<pad>': 57, '<s>': 58, '</s>': 59\n",
        "# }\n",
        "\n",
        "char_mapping = {' ': 0,\n",
        "    'ا': 1, 'ب': 2, 'ت': 3, 'ث': 4, 'ج': 5, 'ح': 6, 'خ': 7, 'د': 8, 'ذ': 9, 'ر': 10, 'ز': 11, 'س': 12, 'ش': 13, 'ص': 14,\n",
        "    'ض': 15, 'ط': 16, 'ظ': 17, 'ع': 18, 'غ': 19, 'ف': 20, 'ق': 21, 'ك': 22, 'ل': 23, 'م': 24, 'ن': 25, 'ه': 26, 'و': 27,\n",
        "    'ى': 28, 'ي': 29,'ء': 30, 'آ': 31, 'أ': 32, 'ؤ': 33, 'إ': 34, 'ئ': 35,'ة': 36,\n",
        "    '٠': 37, '١': 38, '٢': 39, '٣': 40, '٤': 41, '٥': 42, '٦': 43, '٧': 44, '٨': 45, '٩': 46,\n",
        "    '0': 47, '1': 48, '2': 49, '3': 50, '4': 51, '5': 52, '6': 53,'7': 54, '8': 55, '9': 56,\n",
        "    '<pad>': 57, '<s>': 58, '</s>': 59,\n",
        "   '.':60,',':61,'،': 62,':':63,';':64,'؛':65,'(':66,')':67,'[': 68,']':69,'{': 70,'}': 71,'«': 72,'»': 73,'-': 74, '!': 75, '?': 76,'؟': 77,\n",
        "    '\\n': 78, '\"': 79, '&': 80, \"'\": 81, '*': 82, '+': 83, '/': 84, '=': 85,  '_': 86, '`': 87, '~': 88,'\\u200d': 89, '\\u200f': 90, '–': 91,\n",
        "    '’': 92, '“': 93, '…': 94, '﴾': 95, '﴿': 96\n",
        "}\n",
        "\n",
        "class_mapping = {'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ':\n",
        "9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}\n",
        "\n",
        "reverse_class_mapping = {0:'َ', 1:'ً', 2:'ُ', 3:'ٌ', 4:'ِ', 5:'ٍ', 6:'ْ',7:'ّ',8: 'َّ',9: 'ًّ',10: 'ُّ',11: 'ٌّ',12: 'ِّ',13: 'ٍّ',14: ''}\n",
        "\n",
        "\n",
        "punctionations_splitting ={'.':'.\\n',',':',\\n','،': '،\\n',':':':\\n',';':';\\n','؛':'؛\\n','(':'\\n(',')':')\\n',\n",
        "                           '[': '\\n[',']':']\\n','{': '\\n{','}': '}\\n','«': '\\n«','»': '»\\n',\n",
        "                           '-': '-\\n', '!': '!\\n', '?': '?\\n', '؟': '؟\\n',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSGjfNqsufow",
        "outputId": "6239948a-dff0-47c4-9ee2-13754ec1b371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "قوله : ( أو قطع الأول يده إلخ ) قال الزركشي\n"
          ]
        }
      ],
      "source": [
        "def remove_diacritics(data):\n",
        "    return data.translate(str.maketrans('', '', ''.join(diacritics)))\n",
        "\n",
        "test_str = 'قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ'\n",
        "print(remove_diacritics(test_str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vndJkMMtusfV",
        "outputId": "5c375bd4-aee6-4c57-95b5-c23746db6f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
          ]
        }
      ],
      "source": [
        "def one_hot_matrix(data, size):\n",
        "    one_hot_matrix = [[1 if j == i else 0 for j in range(size)] for i in data]\n",
        "    return one_hot_matrix\n",
        "\n",
        "test = [0,1,2,3]\n",
        "print(one_hot_matrix(test, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh7FrHSIusiN",
        "outputId": "cc772b0e-6fb1-43d6-e1b6-b58fda930f92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def one_hot_vector(index , size):\n",
        "    one_hot_vector = [1 if j == index else 0 for j in range(size)]\n",
        "    return one_hot_vector\n",
        "\n",
        "one_hot_vector(1,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20NPdg4Jusk9",
        "outputId": "ba95734c-21b2-426f-dd0a-156dc38a8e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['قَوْلُهُ :', ' ', '( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ )', ' قَالَ الزَّرْكَشِيُّ', '( 14 / 123 )', '', '', 'ابْنُ عَرَفَةَ :', ' قَوْلُهُ :', ' بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ', '( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ )', ' ابْنُ عَرَفَةَ :', ' قَوْلُ ابْنِ شَاسٍ :', ' أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ', '( وَسِحْرٍ )', ' مُحَمَّدٌ :', ' قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ :', ' هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ .', '', '']\n"
          ]
        }
      ],
      "source": [
        "def split_using_punctuation(data):\n",
        "\n",
        "  splitted_data = list()\n",
        "\n",
        "  for sentence in data:\n",
        "        for punc in punctionations_splitting:\n",
        "          sentence = sentence.replace(punc, punctionations_splitting[punc])\n",
        "        splitted_data += sentence.split('\\n')\n",
        "\n",
        "  return splitted_data\n",
        "\n",
        "print(split_using_punctuation(train_data[0:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AXXCgm2Rusoh"
      },
      "outputs": [],
      "source": [
        "def split_on_length(data):\n",
        "\n",
        "    max_len = 500\n",
        "\n",
        "    splitted_data = list()\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "       new_sentence = remove_diacritics(sentence).strip()\n",
        "\n",
        "       if len(new_sentence) != 0:\n",
        "\n",
        "          if len(new_sentence) > 0 and len(new_sentence) <= max_len:\n",
        "                  splitted_data.append(sentence.strip())\n",
        "\n",
        "          else:\n",
        "            sentence_words = sentence.split()\n",
        "            temp_sentence = ''\n",
        "\n",
        "            for word in sentence_words:\n",
        "\n",
        "\n",
        "              # if we add the word, it will exceed length, so don't add this word and take the sentence\n",
        "              if len(remove_diacritics(temp_sentence).strip()) + len(remove_diacritics(word).strip()) + 1 > max_len:\n",
        "                  if len(remove_diacritics(temp_sentence).strip()) > 0:\n",
        "                      splitted_data.append(temp_sentence.strip())\n",
        "\n",
        "                  # make a new sentence\n",
        "                  temp_sentence = word\n",
        "\n",
        "              else:\n",
        "                  # it will not exceed, add the word to the sentence\n",
        "                  temp_sentence = word if temp_sentence == '' else temp_sentence + ' ' + word\n",
        "\n",
        "            if len(remove_diacritics(temp_sentence).strip()) > 0:\n",
        "                  splitted_data.append(temp_sentence.strip())\n",
        "\n",
        "    return splitted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_tYb1WMus81",
        "outputId": "7bd8cb1c-dce0-461f-cd17-ecbf303c35c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data length: 305772\n",
            "Validation data length: 15701\n",
            "['قَوْلُهُ :', '( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ )', 'قَالَ الزَّرْكَشِيُّ', '( 14 / 123 )', 'ابْنُ عَرَفَةَ :']\n"
          ]
        }
      ],
      "source": [
        "split_punctuation_train_data = split_using_punctuation(train_data)\n",
        "split_length_train_data      = split_on_length(split_punctuation_train_data)\n",
        "\n",
        "split_punctuation_val_data = split_using_punctuation(val_data)\n",
        "split_length_val_data      = split_on_length(split_punctuation_val_data)\n",
        "\n",
        "\n",
        "print('Training data length:', len(split_length_train_data))\n",
        "print('Validation data length:', len(split_length_val_data))\n",
        "\n",
        "print(split_length_train_data[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1oJP26F6zUs",
        "outputId": "b4fe4d54-de34-4ebf-93e2-6167e61c164a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data max: 500\n",
            "Validation data max: 500\n",
            "Training data min: 1\n",
            "Validation data min: 1\n"
          ]
        }
      ],
      "source": [
        "print('Training data max:', max(len(remove_diacritics(item).strip()) for item in split_length_train_data))\n",
        "print('Validation data max:', max(len(remove_diacritics(item).strip()) for item in split_length_val_data))\n",
        "\n",
        "print('Training data min:', min(len(remove_diacritics(item).strip()) for item in split_length_train_data))\n",
        "print('Validation data min:', min(len(remove_diacritics(item).strip()) for item in split_length_val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzmpvsrQ7FX8",
        "outputId": "5acdb8ef-4b18-4dfc-80d9-a951f17d82db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data length: 280228\n",
            "Validation data length: 14385\n",
            "['قَوْلُهُ', 'أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ', 'قَالَ الزَّرْكَشِيُّ', 'ابْنُ عَرَفَةَ', 'قَوْلُهُ']\n"
          ]
        }
      ],
      "source": [
        "# list of short sentences -> with diarictic & without punc or numbers\n",
        "clean_diac_train_data = [(''.join(char for char in text if char in arabic_chars_valid)).strip() for text in split_length_train_data]\n",
        "clean_diac_val_data = [(''.join(char for char in text if char in arabic_chars_valid)).strip() for text in split_length_val_data]\n",
        "\n",
        "clean_diac_train_data = [item for item in clean_diac_train_data if item != \"\"]\n",
        "clean_diac_val_data   = [item for item in clean_diac_val_data if item != \"\"]\n",
        "\n",
        "print('Training data length:', len(clean_diac_train_data))\n",
        "print('Validation data length:', len(clean_diac_val_data))\n",
        "\n",
        "print(clean_diac_train_data[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8P_CYiS7FhM",
        "outputId": "c9f5744b-875d-4227-95fb-777db5a448b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data length: 280228\n",
            "Validation data length: 14385\n",
            "['قوله', 'أو قطع الأول يده إلخ', 'قال الزركشي', 'ابن عرفة', 'قوله']\n"
          ]
        }
      ],
      "source": [
        "# list of short sentences -> without diarictic & without punc or numbers\n",
        "\n",
        "clean_train_data = [remove_diacritics(text) for text in clean_diac_train_data]\n",
        "clean_val_data = [remove_diacritics(text) for text in clean_diac_val_data]\n",
        "\n",
        "print('Training data length:', len(clean_train_data))\n",
        "print('Validation data length:', len(clean_val_data))\n",
        "\n",
        "print(clean_train_data[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ms8hsLwk6Z0i"
      },
      "outputs": [],
      "source": [
        "# word embeddings\n",
        "# feature extraction using word2vec\n",
        "\n",
        "def train_word_embeddings(docs):\n",
        "\n",
        "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(docs)\n",
        "\n",
        "    sentences = [doc.split() for doc in docs ]\n",
        "    sentences.append(['<unk>'])\n",
        "    word2vec_model = Word2Vec(sentences, vector_size = 300, window=5, min_count=1, workers=4)\n",
        "\n",
        "    word_embeddings = word2vec_model.wv\n",
        "\n",
        "    return word_embeddings, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "65TKwiC36Z7E"
      },
      "outputs": [],
      "source": [
        "data_to_embeddings = clean_train_data + clean_val_data\n",
        "data_embeddings, tokenizer = train_word_embeddings(data_to_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CdNMhP3b6Z-r"
      },
      "outputs": [],
      "source": [
        "def get_word_embeddings(word):\n",
        "\n",
        "    encoded_docs = tokenizer.texts_to_sequences(word)\n",
        "    # print(encoded_docs)\n",
        "\n",
        "    word_embeddings_for_sample = [data_embeddings[word_index] for word_index in encoded_docs[0] if word_index in data_embeddings]\n",
        "    # print( word_embeddings_for_sample)\n",
        "    return word_embeddings_for_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4UeY0kcB6kjK"
      },
      "outputs": [],
      "source": [
        "# print(len(get_word_embeddings(['إلخ'])))\n",
        "# print(len(get_word_embeddings('إلخ')))\n",
        "\n",
        "# print(len(get_word_embeddings(['<s>'])[0]))\n",
        "\n",
        "# print(len(get_word_embeddings(['لا'])))\n",
        "# print(len(get_word_embeddings('لا')))\n",
        "\n",
        "# print(len(get_word_embeddings(['و'])))\n",
        "# print(len(get_word_embeddings(['<s>'])))\n",
        "# print(len(get_word_embeddings([' '])))\n",
        "# print(len(get_word_embeddings(['<space>'])))\n",
        "# # print(get_word_embeddings(['<s>'])[0])\n",
        "# print(len(get_word_embeddings(['<s>'])[0]))\n",
        "# print(len(get_word_embeddings(['<unk>'])[0]))\n",
        "\n",
        "# print(len(get_word_embeddings('/')))\n",
        "\n",
        "# text = remove_diacritics('سَامِرِيًّا')\n",
        "# print(text)\n",
        "\n",
        "\n",
        "# print(get_word_embeddings(remove_diacritics('سَامِرِيًّا')))\n",
        "# print(get_word_embeddings([remove_diacritics('سَامِرِيًّا')]))\n",
        "\n",
        "# print(len(get_word_embeddings(remove_diacritics('سَامِرِيًّا'))))\n",
        "# print(len(get_word_embeddings([remove_diacritics('سَامِرِيًّا')])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tdllb9NQ-5Z",
        "outputId": "9161dc4f-e7ad-4af7-feec-3d9342bea051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.067544736, 0.76709205, 0.5840071, 1.6572703, 0.12215119, -0.36920074, -0.2765186, 0.12007328, 0.27583122, 0.053990763, 1.0635926, 0.41939303, 0.41312182, 0.38595146, 0.8383707, -0.4555174, 1.1895843, -0.61692226, -0.064696155, 0.5508448, -0.51671696, -0.5667394, -0.12985371, 0.65696883, -0.2141216, 0.0015831317, -0.40760818, 0.75047535, 0.30182603, 0.1320072, 1.4409359, -1.0610815, 0.4466409, -0.2100944, -0.49410155, 0.4610146, 0.6644944, 0.062507845, -0.58480513, 0.7691796, -0.20447914, 0.9006404, 0.55563617, -0.25192878, -0.0038907698, 0.93701714, -0.8276705, 1.025704, 0.2670504, 1.6268004, 0.007654873, 1.2094772, -0.87630755, -0.2682134, 0.20475933, 1.0067306, -0.276556, 1.1657456, 0.48431954, -0.20487687, -0.36072576, 0.5682315, 1.0407214, -0.70266664, -0.77749646, -0.95201045, -0.71015227, 1.62491, -0.53109825, -0.7469156, -0.12789412, -0.2904271, -0.84085846, 0.38829714, -0.8900236, -0.62140805, -0.943815, 1.1940216, 0.3115169, 0.08464387, 1.6119293, 0.18730327, 0.4802861, 1.6077552, -0.6076207, -0.30596948, 0.85178185, 1.3302382, 0.22346458, -0.6159274, 0.5413668, 0.07792549, -0.60562515, 0.7421962, 1.698134, 0.8998585, 0.8312262, -0.45239204, -1.672299, -0.26736498, -0.32576698, -0.45334384, 0.77252376, -0.22776112, -0.35396746, 0.6644694, 0.43161702, -0.12969893, -0.5803307, -0.64422125, -0.08896776, 0.6356638, 0.58851075, -0.022839496, 0.22404945, -0.32541028, -1.0100101, -0.61155856, 0.7868056, -0.63998216, -0.26677105, -0.0923951, 1.7508067, 0.35492796, 0.8874368, -0.17510045, -0.034689657, 0.93038404, 0.48064625, -0.0054444843, 0.8073528, -0.32092056, 0.52276075, -0.615046, -0.08319129, -0.2473331, -0.6498771, -0.5203338, -0.9148589, 0.29313836, 0.68609774, -0.30784255, -1.2949713, 1.1632016, 0.009864666, -1.8147763, -0.17050394, 0.3156049, 0.17738539, -0.017708698, 0.3850787, -0.9564216, -0.08827024, 0.39636904, -0.36646008, -0.27227247, -1.3437887, -1.9216223, 0.7217036, 0.20009184, 0.24652605, -0.80473185, -0.036348995, 0.9398615, -1.1800723, -0.7065444, -0.06015764, -0.06374411, -0.43133757, 1.2995797, -0.58330476, 0.5711239, -0.19488274, 0.78935015, -0.8014255, 0.94752836, -0.5810828, -0.60095847, -0.3819593, 0.48026365, -1.4012022, -0.16798942, 0.5793112, -0.2623058, -0.38759997, -0.71369445, 0.5467089, -0.7613417, 0.0039182627, -0.10144511, 0.1794647, -0.54818565, 0.07810239, -0.131448, -0.23564719, -0.40059063, 1.0719084, 0.8314037, -0.87033516, -1.0717843, -0.55053955, 0.4833782, 0.6522991, -0.06597442, -0.6530237, 0.017991947, -0.028509546, 0.6302025, -0.40382105, -1.7960924, -1.0082489, -1.1888396, 0.005771843, -0.23886675, -0.49720803, -0.43172857, 0.544463, -0.7973435, -0.48712867, -0.41489175, -2.095225, -0.40068483, 0.7931319, 0.6909916, 0.21006344, -0.08565465, 0.009793636, -1.145297, 0.061162535, 0.5195428, 0.22338204, -0.07939432, 0.31595176, -0.88552266, -0.26286235, 0.38625368, 0.3108256, 0.4766358, 0.63050896, -0.24662966, 1.4590087, -1.3205447, -1.6664643, 0.5637667, -1.6060574, -0.7455115, -0.4305232, 0.60472137, -0.4009708, -0.25500435, -0.22958921, -1.3414248, 0.11533771, 0.6815081, -1.7287484, -0.31255275, 0.78698874, 0.12034089, -0.16347164, -0.6202472, -0.5215027, 0.11286061, 0.779206, -1.6304207, -0.70043015, 0.2067051, -0.03587262, 1.2065632, -0.14816128, -0.98700696, 0.43677965, 0.6036437, -0.8449106, 0.49613118, 1.6990273, 1.4068941, -1.1850731, 0.6926676, -0.25497004, -0.16165625, 1.4289457, -0.85431844, -0.12976313, -0.162193, 0.25008413, -0.59304017, -0.8026227, 0.4078161, -0.6814305, 1.3487089, 0.16619375, -0.07522616, -1.1812263, 1.1351501, 0.5087307, 0.8572676, 1.438748, 0.228742, -0.064084575, -0.13497357]\n",
            "397\n"
          ]
        }
      ],
      "source": [
        "vec = one_hot_vector(char_mapping['<s>'],len(char_mapping))\n",
        "vec.extend(get_word_embeddings(['<s>'])[0])\n",
        "print(vec)\n",
        "print(len(vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBhHYDTs178q",
        "outputId": "700f2469-ba49-4923-f84b-c89050e8eaa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# print(get_word_embeddings(['<unk>'])[0])\n",
        "# print(get_word_embeddings(remove_diacritics('قوله'))[0])\n",
        "print(len(get_word_embeddings(remove_diacritics('قوله'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VsQRnTjW9u97"
      },
      "outputs": [],
      "source": [
        "def get_sentence_classes(sentence):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  unk_emb = get_word_embeddings(['<unk>'])[0]\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping['<s>'],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  for word in sentence.split():\n",
        "\n",
        "    emb2 = get_word_embeddings(remove_diacritics(word))\n",
        "\n",
        "    if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "    else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    if word in punctionations_splitting:\n",
        "      emb = unk_emb\n",
        "    else:\n",
        "      if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "      else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "\n",
        "    for index, char in enumerate(word):\n",
        "\n",
        "      if char not in diacritics: # arabic char or space\n",
        "\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[char],len(char_mapping))\n",
        "        vec.extend(emb)\n",
        "        x.append(vec)\n",
        "\n",
        "        char_diacritic = ''\n",
        "        sentence_len = len(sentence)\n",
        "\n",
        "        if index + 1 < sentence_len:\n",
        "          if sentence[index + 1] in diacritics:\n",
        "            char_diacritic = sentence[index + 1]\n",
        "\n",
        "            if index + 2 < sentence_len:\n",
        "               char_diacritic = char_diacritic + sentence[index + 2] if sentence[index + 2] in diacritics and (char_diacritic + sentence[index + 2] in class_mapping) else sentence[index + 2] + char_diacritic if sentence[index + 2] in diacritics and (sentence[index + 2] + char_diacritic in class_mapping) else char_diacritic\n",
        "\n",
        "\n",
        "        y.append(one_hot_vector(class_mapping[char_diacritic],len(class_mapping)))\n",
        "\n",
        "    # vec = []\n",
        "    # vec = one_hot_vector(char_mapping[' '],len(char_mapping))\n",
        "    # vec.extend(unk_emb)\n",
        "    # x.append(vec)\n",
        "    # y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping['</s>'],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  assert(len(x) == len(y))\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "to-lj1YqRrip"
      },
      "outputs": [],
      "source": [
        "def get_sentence_classes_test(sentence):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  unk_emb = get_word_embeddings(['<unk>'])[0]\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping['<s>'],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  for word in sentence.split():\n",
        "\n",
        "    emb2 = get_word_embeddings(remove_diacritics(word))\n",
        "\n",
        "    if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "    else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    if word in punctionations_splitting:\n",
        "      emb = unk_emb\n",
        "    else:\n",
        "      if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "      else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "\n",
        "    for index, char in enumerate(word):\n",
        "\n",
        "      if char not in diacritics: # arabic char or space\n",
        "\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[char],len(char_mapping))\n",
        "        vec.extend(emb)\n",
        "        x.append(vec)\n",
        "\n",
        "        char_diacritic = ''\n",
        "        sentence_len = len(sentence)\n",
        "\n",
        "        if index + 1 < sentence_len:\n",
        "          if sentence[index + 1] in diacritics:\n",
        "            char_diacritic = sentence[index + 1]\n",
        "\n",
        "            if index + 2 < sentence_len:\n",
        "              char_diacritic = char_diacritic + sentence[index + 2] if sentence[index + 2] in diacritics and (char_diacritic + sentence[index + 2] in class_mapping) else sentence[index + 2] + char_diacritic if sentence[index + 2] in diacritics and (sentence[index + 2] + char_diacritic in class_mapping) else char_diacritic\n",
        "\n",
        "\n",
        "        y.append(one_hot_vector(class_mapping[char_diacritic],len(class_mapping)))\n",
        "\n",
        "    vec = []\n",
        "    vec = one_hot_vector(char_mapping[' '],len(char_mapping))\n",
        "    vec.extend(unk_emb)\n",
        "    x.append(vec)\n",
        "    y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping['</s>'],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  assert(len(x) == len(y))\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8JiiRaP69vE8"
      },
      "outputs": [],
      "source": [
        "def get_classes(data):\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for sentence in data:\n",
        "    x, y = get_sentence_classes(sentence)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = np.asarray(X)\n",
        "  Y = np.asarray(Y)\n",
        "\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LKSs8_81S3Ig"
      },
      "outputs": [],
      "source": [
        "def get_classes_test(data):\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for sentence in data:\n",
        "    x, y = get_sentence_classes_test(sentence)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = np.asarray(X)\n",
        "  Y = np.asarray(Y)\n",
        "\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoD49_mQFR_4",
        "outputId": "84ffab07-fd01-4dea-8b14-688d44ae18d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['قَوْلُهُ', 'أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ']\n",
            "(2,)\n",
            "(2,)\n",
            "6\n",
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        }
      ],
      "source": [
        "print(clean_diac_train_data[0:2])\n",
        "\n",
        "X,Y = get_classes(clean_diac_train_data[0:2])\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(len(X[0]))\n",
        "print(len(Y[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a3uKzrFn7FqL"
      },
      "outputs": [],
      "source": [
        "class custom_data_generator(Sequence):\n",
        "\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = (index + 1) * self.batch_size\n",
        "\n",
        "        batch = self.data[start_index : end_index]\n",
        "        X_batch, Y_batch = get_classes(batch)\n",
        "\n",
        "        max_length_X = np.max([len(x) for x in X_batch])\n",
        "        max_length_Y = np.max([len(y) for y in Y_batch])\n",
        "\n",
        "        assert(max_length_X == max_length_Y)\n",
        "\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping['<pad>'],len(char_mapping))\n",
        "        vec.extend(get_word_embeddings(['<pad>'])[0])\n",
        "\n",
        "        X = []\n",
        "        for x in X_batch:\n",
        "          padding_length = max_length_X - len(x)\n",
        "          x = list(x)\n",
        "          x.extend([vec] * (padding_length))\n",
        "          X.append(np.asarray(x))\n",
        "\n",
        "        Y = []\n",
        "        for y in Y_batch:\n",
        "          padding_length = max_length_Y - len(y)\n",
        "          y = list(y)\n",
        "          # y.extend([one_hot_vector(class_mapping['<pad>'],len(class_mapping))] * (padding_length))\n",
        "          y.extend(one_hot_matrix([class_mapping['']] * (padding_length), len(class_mapping)))\n",
        "          Y.append(np.asarray(y))\n",
        "\n",
        "        X, Y = np.asarray(X), np.asarray(Y)\n",
        "\n",
        "        # print('===================================> X:', X.shape)\n",
        "        # print('===================================> Y:', Y.shape)\n",
        "\n",
        "        return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QJcefM6QSFRa"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "   model = Sequential()\n",
        "   model.add(InputLayer(input_shape=(None, 397)))\n",
        "\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=512,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=512,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=len(class_mapping),activation='softmax',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKdOlRTsUvfN",
        "outputId": "56489fd4-2c6c-471f-9f36-e3a7ac164215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, None, 512)         1339392   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 512)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, None, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 512)         0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, None, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, None, 512)         262656    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, None, 512)         262656    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 15)          7695      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5022223 (19.16 MB)\n",
            "Trainable params: 5022223 (19.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "znC0M3mTU-cz"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, epochs, batch_size, train_data, val_data):\n",
        "\n",
        "    random.shuffle(train_data)\n",
        "    random.shuffle(val_data)\n",
        "\n",
        "    train_data = list(sorted(train_data, key=lambda item: len(remove_diacritics(item))))\n",
        "    val_data   = list(sorted(val_data,   key=lambda item: len(remove_diacritics(item))))\n",
        "\n",
        "    checkpoint_path = '/content/drive/My Drive/NLPdata/checkpoints/epoch{epoch:02d}.ckpt'\n",
        "\n",
        "    checkpoint_cb = ModelCheckpoint(checkpoint_path, verbose=0)\n",
        "\n",
        "    training_generator = custom_data_generator(train_data, batch_size)\n",
        "    val_generator = custom_data_generator(val_data, batch_size)\n",
        "\n",
        "    history =  model.fit(training_generator,validation_data=val_generator,epochs=epochs,callbacks=[checkpoint_cb])\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUVM7ELggEE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puI4izsfggf5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0NWU4CU-8y",
        "outputId": "93ff3c6f-215d-47ca-a344-28e6eab082f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1095/1095 [==============================] - 1189s 1s/step - loss: 0.8396 - accuracy: 0.6741 - val_loss: 0.4772 - val_accuracy: 0.8361\n",
            "Epoch 2/5\n",
            "   1/1095 [..............................] - ETA: 4:11 - loss: 0.5867 - accuracy: 0.8114"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1095/1095 [==============================] - 1162s 1s/step - loss: 0.3982 - accuracy: 0.8530 - val_loss: 0.2900 - val_accuracy: 0.9008\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1095/1095 [==============================] - 1162s 1s/step - loss: 0.2746 - accuracy: 0.9006 - val_loss: 0.2256 - val_accuracy: 0.9254\n",
            "Epoch 4/5\n",
            "   1/1095 [..............................] - ETA: 3:41 - loss: 0.1559 - accuracy: 0.9486"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1095/1095 [==============================] - 1202s 1s/step - loss: 0.2228 - accuracy: 0.9208 - val_loss: 0.1911 - val_accuracy: 0.9382\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-0c49e0fe44f8>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-23-0c49e0fe44f8>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1095/1095 [==============================] - 1202s 1s/step - loss: 0.1925 - accuracy: 0.9328 - val_loss: 0.1720 - val_accuracy: 0.9457\n",
            "Final Training Accuracy: 0.9327805638313293\n",
            "Final Validation Accuracy: 0.9457154273986816\n",
            "5980.79 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "history =fit_model(model, 5, 256, clean_diac_train_data, clean_diac_val_data)\n",
        "end_time = time.time()\n",
        "\n",
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "print('Final Training Accuracy:', training_accuracy[-1])\n",
        "print('Final Validation Accuracy:', validation_accuracy[-1])\n",
        "\n",
        "print('%s seconds' % round(end_time - start_time, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8JaQiboyQwMU"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model, 'word2vec_new2.joblib')\n",
        "filename = 'word2vec_new2.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
